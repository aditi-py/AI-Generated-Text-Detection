# -*- coding: utf-8 -*-
"""AI-generated text detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rPCZUtFddY4G5phlG5uN45R2ZMqXrlza

# Project: AI-generated text detection ðŸ¤–
--------------------------------

Can you predict the 'ind' (0= human, 1 = AI) as a function of the 768 document embeddings, the word count and the punctuation? Be careful - there is imbalanced data which may require some advanced methods in order to get a reasonable model. Can you beat a naive (majority class) model or logistic regression?

# ðŸ”´ Importing Libraries
"""

# import modules
# for general data analysis/plotting
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns

# for data prep
from sklearn.model_selection import train_test_split

#for standardscaler
from sklearn.preprocessing import StandardScaler

#for classification report and confusion matrix
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report

# neural net modules
from keras.models import Sequential
from keras.layers import Dense, Dropout
from keras.callbacks import EarlyStopping

"""# ðŸ”´ Downloading and importing Data
* Raw data was download from this website:
  * https://bitgrit.net/competition/19



"""

!pip install --upgrade gdown

# https://drive.google.com/file/d/1mdAU_vvGkDZTIcsU8QVUez3RLV4KDuDf/view?usp=sharing
!gdown 1mdAU_vvGkDZTIcsU8QVUez3RLV4KDuDf

# unzip it
!unzip ai-text-competition-data.zip

# read it
df = pd.read_csv('/content/ai-text-competition/training_set.csv')

"""## ðŸŸ¡ Exploring Data"""

#Checcking the shape of data
print(df.shape)

# Checking first few samples of the data
df.head()

"""### ðŸ”µ Summary of Dataset"""

# Quick summary of the dataset
df.describe()

"""### ðŸ”µ Distribution of Target Variable"""

# checking the distribution of target variable
df['ind'].value_counts()

"""--> The number of 1 is almost 10% of the complete dataset. If the future model does not perform well without any sampling technique, this will help us to use any of the sampling techniques.

# ðŸ”´ Cleaning Data

## ðŸŸ¡ Checking DataType
"""

# Checking the Datatype
df.info()

#Confirming the datatype
df.dtypes

"""--> The data type for all the predictor variable looks good. So not making any conversion for the data types of the varible

## ðŸŸ¡ Checking for Null Values
"""

#Checking Null values for each column in the dataset
df.isnull().sum()

#Confirming the total null values in the dataframe
df.isnull().sum().sum()

"""--> There are no null values in the complete dataset. The dat set is clean and does not require any cleaning

# ðŸ”´ Exploratory Data Analysis

## ðŸ’›  Plot 1 - HeatMap for Predictor variables with Correlation > 0.15 with Target Variable
"""

# Calculating the correlation matrix
corr = df.corr()

# Assgning 'ind' to target variable
target_variable = 'ind'

# Filtering variables that have correlation greater than 0.15 (positive or negative) with the target variable
correlated_vars = corr[(corr[target_variable] > 0.15) | (corr[target_variable] < -0.15)].index

# Extracting the correlation matrix for the correlated variables
correlated_matrix = df[correlated_vars].corr()

# size of the plot
plt.figure(figsize=(16, 12))

# Creating a heatmap for the correlated variable
sns.heatmap(correlated_matrix, annot=True, cmap='coolwarm', fmt=".2f")

# Displaying the heatmap

plt.title("Correlation Heatmap for Variables with Correlation > 0.15 with 'ind'")
plt.show()

"""1.  The below figure will give an understanding of the predictor variables having an strong correlation with the Target Variable.
2. 17 predictor variables have a very good correlation with a value greater than 0.15(strongly and negatively correlated)
3. These seventeen variables will be used to make a model in the future to see how the model performs with only seventeen predictor variables while comparing it to a model with all the predictor variables(771) (Ind will be excluded from all the models in future)

## ðŸ§¡ Plot 2 - Distribution of word_count & punc_num with ind
"""

# Selecting the required variables
selected_variables = ['word_count', 'punc_num', 'ind']
selected_df = df[selected_variables]


# size of the figure
fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))

# Creating histogram for 'word_count' with respect to 'ind'
sns.histplot(data=selected_df, x='word_count', hue='ind', kde=True, ax=axes[0], palette='viridis')
axes[0].set_title('Distribution of word_count with respect to ind')

# Creating histogram for 'punc_num' with respect to 'ind'
sns.histplot(data=selected_df, x='punc_num', hue='ind', kde=True, ax=axes[1], palette='viridis')
axes[1].set_title('Distribution of punc_num with respect to ind')

# Adjusting the layout
plt.tight_layout()

# Display the plots
plt.show()

"""1. As all the predictor variables are document embeddings only word count and punctuation are a little different set of predictors. So checking the skewness of those predictor variables on the target variable.
2. Also punctuations has a very good correlation of -0.21 in the plot1 heatmap, so the distribution of the punctuation will give a good understanding of its relationship with the target variable ind. The heatmap and distribution plot indicates punctaution will be one of the important predictor variable in our models.

## ðŸ’œ plot 3 - Distribution of the Target variable
"""

#Distribution of target variable
plt.figure(figsize=(8, 6))
sns.countplot(x='ind', data=df)
plt.title('Distribution of ind')
plt.show()

"""The distribution of target variable is a simple plot. But this will give us a visual representation of the target variables and will motivate us to use sampling techniques in the future if any of our models doesnt give good results without sampling.

## ðŸ’— Table-1 showing the relationship between puntuations, wordcount and AI
"""

# Filtering all the AI based on the condition punctuation less than average punctuation and word count is less than the average word count
tmp_df = df[(df['punc_num'] < 7) & (df['word_count'] < 30) & (df['ind'] == 1)]

tmp_df.head()

tmp_df['ind'].value_counts()

"""--> The above table will have a list of all the AI models based on the condition the number of punctuations is less than the average punctuation and the word count is less than the average number of word counts.

--> It is clear that there is a very less number of AI when the punctation and word count is less than the average number.

--> There are only 514 AI models which satisfy the above condition which is a very low number

## ðŸ’– Table 2 - Patterns in Punctuation and word count
"""

# Exploring when punctaions and word count both are even number

tmp_df2 = df[(df['punc_num'] % 2 == 0) & (df['word_count'] % 2 == 0)]
tmp_df2.head()

tmp_df2['ind'].value_counts()

"""Surprisingly, when both word counts and punctuation counts are even numbers, the dataset exhibits a notable imbalance in favor of human observations. Specifically, there are 2589 instances of humans compared to only 252 instances of AI. The occurrences of AI represent merely 10% of the human observations, highlighting a substantial disparity in the dataset composition."""

# Exploring when punctaions and word count both are odd numbers(lets see if we can find any pattern with odd count)
tmp_df3 = df[(df['punc_num'] % 2 != 0) & (df['word_count'] % 2 != 0)]
tmp_df3.head()

tmp_df3['ind'].value_counts()

"""Remarkably, when both word counts and punctuation counts are odd numbers, a conspicuous imbalance in favor of human observations emerges in the dataset. Notably, there are 2432 instances of humans compared to a mere 282 instances of AI. The occurrences of AI account for only 10% of the human observations, underscoring a substantial asymmetry in the composition of the dataset.

The Pattern found in both using odd and even counts can be used in the future for future enginnering. This was a very interesting observation

## ðŸ”´ Plot 4 - Plot for Punctuation and word count
"""

# word count is even and punctuation count is odd
tmp_df4 = df[(df['word_count'] % 2 == 0) & (df['punc_num'] % 2 != 0)]

# word count is odd and punctuation count is even
tmp_df5 = df[(df['word_count'] % 2 != 0) & (df['punc_num'] % 2 == 0)]

# plot1
plt.figure(figsize=(6, 4))
sns.countplot(x='ind', data=tmp_df4, palette='viridis')
plt.title('Distribution of Humans and AI with Even Word Count and Odd Punctuation Count')
plt.xlabel('Class (ind)')
plt.ylabel('Count')
plt.show()

# Plot2
plt.figure(figsize=(6, 4))
sns.countplot(x='ind', data=tmp_df5, palette='viridis')
plt.title('Distribution of Humans and AI with Odd Word Count and Even Punctuation Count')
plt.xlabel('Class (ind)')
plt.ylabel('Count')
plt.show()

"""Upon examining tables for even and odd punctuation counts, my focus was on discerning the patterns in the distribution of humans and AI based on two specific conditions: even word count and odd punctuation count, and odd word count and even punctuation count. In the former scenario, I observed a distinct pattern where the word count was even, and the punctuation count was odd. Surprisingly, there was a noticeable imbalance, with humans significantly outnumbering AI instances. Conversely, in the latter scenario with odd word count and even punctuation count, a similar discernible pattern emerged. Notably, there was a prevalence of human instances, underscoring a consistent and remarkable asymmetry in the distribution of the dataset.

-> The number of Humans in all the conditions will be definitely higher as they are the majority class here. In all these tables and plots the following condition has given the most interesting result for us:tmp_df = df[(df['punc_num'] < 7) & (df['word_count'] < 30) & (df['ind'] == 1)]

# ðŸ”´ Data Partition & Feature Engineering
"""

# Checking the shape for any data leakage
df.shape

#Everything looks good till now

#Dropping ID as we are not going to use it in the Model
#Also dropping ind as it is our target variable

X = df.drop(['ind','ID'], axis=1)
Y = df['ind']

#Reconfirming the shape for X
X.shape

#Reconfirming the shape for Y
Y.shape

# Splitting the data on 90% train and 10% test
X_train, X_test, y_train, y_test = train_test_split(X, Y,
                                                    test_size = 0.1,
                                                    shuffle = True,
                                                    random_state = 42)

#Checking the shape for X_train, X_test, y_train and y_test
print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)

y_test.value_counts()

y_train.value_counts()

#Everything looks good

#Converting to Numpy array
 X_train = np.array(X_train)
 X_test = np.array(X_test)
 y_train = np.array(y_train)
 y_test = np.array(y_test)

print(X_train.shape,X_test.shape,y_train.shape,y_test.shape)

"""## ðŸ”´ Min-Max Scaler"""

# Creating the StandardScaler instance
scaler = StandardScaler()

# Fiting and transforming the training data (X_train)
X_train = scaler.fit_transform(X_train)

# Transforming the test data (X_test) using the same scaler
X_test = scaler.transform(X_test)

"""## ðŸ”´ Feature Importance - Feature Engineering"""

# importing RandomForestClassifier Model

from sklearn.ensemble import RandomForestClassifier #RFC


#Running and Fitting the Model
model = RandomForestClassifier(random_state=42)
model.fit(X_train, y_train)

#feature importances
importances = model.feature_importances_

# sorting the features
sorted_indices = np.argsort(importances)[::-1]

#Selecting the top 85% of the features
n_top_features = int(len(sorted_indices) * 0.85)  # Adjust this to change the number of features
top_feature_indices = sorted_indices[:n_top_features]

# # Select the top features from X_train_scaled and X_test_scaled
# # X_train = X_train[:, top_feature_indices]
# # X_test = X_test[:, top_feature_indices]

"""âŒ Despite my attempts to enhance the model through feature engineering, specifically by selecting the top 85% of features based on their importance in a Random Classifier Model, I did not observe significant improvement compared to the current model that utilizes all features. Experimenting with different percentages of features did not yield substantial enhancements either. I also explored Principal Component Analysis (PCA) to extract features with a strong correlation of 0.15, but this approach did not contribute to better performance. Consequently, I opted to proceed without any feature engineering and retained all the features in the final model.

# ðŸ”´ Modelling

## ðŸŸ¡ Neural Network Model
"""

# Building the Model with dropout of 0.5

#Creating a Sequential Model
model = Sequential()

# Adding first densely connected layer using Relu activation function
model.add(Dense(50, input_shape=(X_train.shape[1],), activation='relu'))

#Adding a dropout layer with dropout rate of 0.5
model.add(Dropout(0.5)) # dropout= 0.5

# Adding second densely connected layer using Relu activation function
model.add(Dense(25, activation='relu'))

#Adding a dropout layer with dropout rate of 0.5
model.add(Dropout(0.5)) # dropout= 0.5

# Adding third densely connected layer using Relu activation function
model.add(Dense(10, activation='relu'))

#Adding a dropout layer with dropout rate of 0.5
model.add(Dropout(0.5)) # dropout= 0.5

#Adding the output layer with sigmoid function
model.add(Dense(1, activation='sigmoid')) # output node = sigmoid

#Displaying the model
model.summary()

"""## ðŸŸ¡ Compiling the Model"""

# compile the model using Adam optimizer and metrics as accuracy
model.compile(optimizer='Adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

"""## ðŸŸ¡ Early Stopping Callback"""

# Early stopping callback with a Patience of 60
es = EarlyStopping(monitor='val_accuracy',
                   mode='max',
                   patience=60,
                   restore_best_weights = True)

"""## ðŸŸ¡ Fitting the Model"""

# Fitting the model
#epochs=100000

history = model.fit(X_train, y_train,
                    validation_data = (X_test, y_test),
                    callbacks=[es],
                    epochs=100000,
                    batch_size=30, # 30 this is a hyperparameter
                    shuffle=True,
                    verbose=1)

"""# ðŸ”´ Evaluation

## ðŸŸ¡ Learning Curves for accuracy and Loss

### ðŸ’œ Plot for Training and Validation Accuracy
"""

# accuracy
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

# loss
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs = range(1, len(acc) + 1)

# "r" is for "solid red line"
plt.plot(epochs, acc, 'r', label='Training accuracy')
# b is for "solid blue line"
plt.plot(epochs, val_acc, 'b', label='Validation accuracy')
plt.title('Training and validation accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

plt.show()

"""### ðŸ’œ Plot for Training and Validation Loss"""

# Training and Validation accuracy by loss


history_dict = history.history
loss_values = history_dict['loss']
val_loss_values = history_dict['val_loss']
epochs = range(1, len(loss_values) + 1) # range of X (no. of epochs)
plt.plot(epochs, loss_values, 'bo', label='Training loss')
plt.plot(epochs, val_loss_values, 'orange', label='Validation loss')
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

"""### Plot for Training and Validation accuracy by epoch"""

# let's see the training and validation accuracy by epoch
history_dict = history.history
loss_values = history_dict['accuracy'] # you can change this
val_loss_values = history_dict['val_accuracy'] # you can also change this
epochs = range(1, len(loss_values) + 1) # range of X (no. of epochs)
plt.plot(epochs, loss_values, 'bo', label='Training acc')
plt.plot(epochs, val_loss_values, 'orange', label='Validation acc')
plt.title('Training and validation accuracy by Epoch')
plt.xlabel('Epochs')
plt.ylabel('Acc')
plt.legend()
plt.show()

"""## ðŸ’› Confusion Matrix and Error Metrics"""

# see how the model did!
preds = model.predict(X)
print(preds.shape)

# i'm spreading that prediction across three nodes and they sum to 1
print(preds[0])

# sum it up! see how probability mass (1) is spread over three preds?
np.sum(preds[0])

"""### âœ… Confusion Matrix for Train Partition"""

# Confusion Matrix for train Partition
# if you don't round to a whole number (0 or 1), the confusion matrix won't work!
preds = np.round(model.predict(X_train),0)

# confusion matrix
confusion_matrix(y_train, preds) # order matters! (actual, predicted)

# TP is bottom right
# TN is top left
# FP is top right
# FN is bottom left

"""Train Partition Results :


     TP - 899

     TN - 9032

     FP - 2

     FN - 96

### âœ… Error Metrics for Train Partition
"""

# Error Metrics for train partition
print(classification_report(y_train, preds))

"""Train Partition - The model achieved 99% precision and  recall for both Class 0 and Class 1, with a 99% F1-score for Class 0 and 95% for Class 1, demonstrating robust classification performance. Class 0, with 100% recall, correctly predicted all Human instances, while Class 1(AI), with 91% recall, showed high accuracy in predicting positive instances. The overall accuracy reached 99%, and macro and weighted averages of precision, recall, and F1-score indicated consistent performance across both classes, accounting for class imbalance.

### âœ… Confusion Matrix for Test Partition
"""

#Confusion Matrix for test Partition
preds = np.round(model.predict(X_test),0)

# confusion matrix
confusion_matrix(y_test, preds) # order matters! (actual, predicted)

# TP is bottom right
# TN is top left
# FP is top right
# FN is bottom left

"""Test Partition Results :


     TP - 65

     TN - 1004

     FP - 6

     FN - 40

### âœ… Error Metrics for Test Partition
"""

#Error Metrics for test Partition

print(classification_report(y_test, preds))

"""Test Partition - The model achieved 96% precision and 99% recall for Human (Class 0), indicating high accuracy in predicting Human instances, while for AI(class-1), it showed 92% precision and 62% recall, resulting in a 74% F1-score. The overall accuracy on the test partition was 96%, with a macro average of 94% for precision and 81% for recall, and a weighted average of 96% for precision, 96% for recall, and 96% for F1-score, demonstrating consistent performance across both classes, albeit with lower recall for Class 1.

# ðŸ˜  Final F1 Score
"""

from sklearn.metrics import f1_score

f1_score(y_test, preds)

"""# ðŸ”¶ Conclusion

1. At the project's initiation, I conducted an initial model run by exclusively choosing predictors with a significant correlation (greater than 0.15, either positive or negative) with the target variable. However, employing a dense layer neural network revealed an F1 score of approximately 0.6 with that model. This initial F1 score prompted the realization that retaining a substantial number of predictor variables in the dataset might be crucial, leading me to reconsider the strategy of dropping a large number of predictors.

2. In my subsequent trial, I explored feature engineering by selecting the top 85% (and various other percentages) of features based on their importance in a Random Classifier Model. However, despite these efforts, the model's performance did not show a significant improvement, maintaining an F1 score ranging between 0.6 and 0.68. Consequently, I opted not to proceed with a model that incorporated feature engineering.

3. Subsequently, I developed a model while retaining all features, comprising three densely connected layers with 50, 25, and 10 neurons, each utilizing ReLU activation functions. After each dense layer, a dropout layer with a dropout rate of 0.5 was incorporated. The output layer employed a sigmoid activation function with one output node. Notably, the model incorporated an early stopping callback with a patience of 60. Ultimately, this model achieved an F1 score of 0.7386 on the test partition.

4. The model demonstrates robust performance in accurately identifying instances of the majority class (Human, Class 0), as evidenced by a high true negative count (1004) and a low false positive count (6). While facing challenges in predicting instances of the minority class (AI, Class 1), reflected in a lower true positive count (65) and a slightly higher false negative count (40), this suggests the model's cautious approach, aiming to minimize false positives, and aligns with the chosen dropout rate's regularization effect during training.

5. An Area Under the Curve (AUC) value of 0.94 in the Receiver Operating Characteristic (ROC) curve indicates strong discriminative power of the model. The closer the AUC is to 1, the better the model distinguishes between positive and negative instances. In this case, the high AUC of 0.94 suggests that the model performs exceptionally well in differentiating between the two classes, affirming its robust predictive capability. (AUC curve is plotted below)

6. Final conclsuion : In the project's early stages, an attempt to select predictors based on correlation yielded a neural network with a modest F1 score of 0.6, prompting reconsideration of feature reduction. Subsequent efforts in feature engineering, despite not significantly improving model performance, informed the decision to retain all features. The final model, a three-layer dense neural network, exhibited robust performance with an F1 score of 0.7386 on the test partition and a strong AUC of 0.94 in the ROC curve, emphasizing its effectiveness in distinguishing between classes. Overall, this model is doing a good job in classifying Human and AI.
"""

import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, roc_auc_score

# Assuming 'model' is your trained neural network
y_probs = model.predict(X_test)
fpr, tpr, thresholds = roc_curve(y_test, y_probs)
auc = roc_auc_score(y_test, y_probs)

plt.plot(fpr, tpr, label=f'AUC = {auc:.2f}')
plt.plot([0, 1], [0, 1], linestyle='--', color='grey')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend()
plt.show()